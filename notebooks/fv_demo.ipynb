{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for '/Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin' at '/Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/modeling_utils.py:556\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    555\u001b[0m     weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: weights_only}\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mweights_only_kwarg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/torch/serialization.py:1539\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1543\u001b[0m     )\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mmap can only be used with files saved with `torch.save(/Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin, _use_new_zipfile_serialization=True), please torch.save your checkpoint with this option in order to use mmap.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/modeling_utils.py:572\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    573\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to locate the file /Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin which is necessary to load this pretrained model. Make sure you have saved the model properly.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEleutherAI/gpt-j-6b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model, tokenizer, model_config \u001b[38;5;241m=\u001b[39m \u001b[43mload_gpt_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m EDIT_LAYER \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/function_vectors_expl/notebooks/../src/utils/model_utils.py:43\u001b[0m, in \u001b[0;36mload_gpt_model_and_tokenizer\u001b[0;34m(model_name, device, revision)\u001b[0m\n\u001b[1;32m     41\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     42\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m---> 43\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m     MODEL_CONFIG\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m:model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_head,\n\u001b[1;32m     46\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m:model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer,\n\u001b[1;32m     47\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresid_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m:model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_embd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_hook_names\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer.h.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)],\n\u001b[1;32m     51\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepend_bos\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-neox\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpythia\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mlower():\n",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/modeling_utils.py:4071\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[1;32m   4069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4070\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[1;32m   4075\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[1;32m   4077\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[1;32m   4078\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/fv/lib/python3.10/site-packages/transformers/modeling_utils.py:577\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    573\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin' at '/Users/karoshi/.cache/huggingface/hub/models--EleutherAI--gpt-j-6b/snapshots/47e169305d2e8376be1d31e765533382721b2cc1/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "EDIT_LAYER = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantonym\u001b[39m\u001b[38;5;124m'\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m mean_activations \u001b[38;5;241m=\u001b[39m get_mean_head_activations(dataset, \u001b[43mmodel\u001b[49m, model_config, tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('antonym', seed=0)\n",
    "mean_activations = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
